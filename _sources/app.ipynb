{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library\n",
    "Kode dibawah digunakan untuk mengimport library yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from statistics import stdev\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "Kode dibawah digunakan untuk membaca file dengan format .csv lalu nantinya ditampilkan dalam bentuk DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [askrl] minta saran skincare buat pemula bgt d...\n",
      "1                      @txtdrstoryWA Geblek ni orang\n",
      "2  abis minum balik terus saur rasanya anjing kud...\n",
      "3  Ini 4th gen it boy gak selese2, gedek jdny, je...\n",
      "4        lagi enak enak puasa trus mens tuh tai bgtt\n"
     ]
    }
   ],
   "source": [
    "TWEET_DATA = pd.read_csv(\"Dataset.csv\", encoding= 'unicode_escape')\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Preprocessing dilakukan untuk menghilangkan noise yang nantinya akan mengganggu saat proses pembelajaran metode machine learning. Adapun tahapan preprocessing yang dilakukan adalah sebagai berikut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Folding\n",
    "Merupakan proses mengubah huruf pada dokumen teks menjadi huruf kecil. Kode dibawah digunakan untuk mengubah huruf alphabet menjadi huruf kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [askrl] minta saran skincare buat pemula bgt d...\n",
      "1                      @txtdrstorywa geblek ni orang\n",
      "2  abis minum balik terus saur rasanya anjing kud...\n",
      "3  ini 4th gen it boy gak selese2, gedek jdny, je...\n",
      "4        lagi enak enak puasa trus mens tuh tai bgtt\n"
     ]
    }
   ],
   "source": [
    "def case_folding(text):\n",
    "    return str.lower(text)\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(case_folding)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Merupakan prosses menghapus karakter – karakter yang tidak diperlukan seperti URL, angka, simbol dan lain sebagainya. Kode dibawah terdapat fungsi yang mana didalamnya terdapat berbagai kode dari python berupa method untuk:\n",
    "- menghapus teks selain kode ascii\n",
    "- menghapus mention, link, hashtag\n",
    "- menghapus angka\n",
    "- menghapus huruf single\n",
    "- menghapus huruf berulang\n",
    "- menghapus tanda baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0   askrl minta saran skincare buat pemula bgt do...\n",
      "1                                    geblek ni orang\n",
      "2  abis minum balik terus saur rasanya anjing kud...\n",
      "3  ini th gen it boy gak selese gedek jdny jelas ...\n",
      "4         lagi enak enak puasa trus mens tuh tai bgt\n"
     ]
    }
   ],
   "source": [
    "def cleaning(text):\n",
    "    #hapus teks yang bukan ascii\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    #hapus mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    #hapus url\n",
    "    text = text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "    #hapus angka\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    #hapus single huruf\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    #hapus huruf berulang\n",
    "    text = ''.join(c for c, _ in groupby(text))\n",
    "    #hapus tanda baca\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in text:\n",
    "        if ele in punc:\n",
    "            text = text.replace(ele, \" \")   \n",
    "    fixtext = re.sub('\\s+',' ',text)\n",
    "    return fixtext\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(cleaning)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi\n",
    "Merupakan proses memecah kalimat menjadi bagian – bagian yang bermakna dan mengidentifikasi entitas individu dalam kalimat. Kode dibawah digunakan untuk mengubah kata dalam suatu teks menjadi token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [askrl, minta, saran, skincare, buat, pemula, ...\n",
      "1                                [geblek, ni, orang]\n",
      "2  [abis, minum, balik, terus, saur, rasanya, anj...\n",
      "3  [ini, th, gen, it, boy, gak, selese, gedek, jd...\n",
      "4  [lagi, enak, enak, puasa, trus, mens, tuh, tai...\n"
     ]
    }
   ],
   "source": [
    "def tokenisasi(text):\n",
    "    return word_tokenize(text)\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(tokenisasi)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisasi\n",
    "Kata yang tidak baku, ambigu atau kata yang diulang – ulang akan diubah menjadi kata yang baku. Kode dibawah digunakan untuk menormalisasi teks dengan mencocokkannya pada sebuah file yang didalamnya terdapat daftar kata sebelum dan sesudah normalisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [tanya, minta, saran, skincare, buat, pemula, ...\n",
      "1                               [geblek, ini, orang]\n",
      "2  [habis, minum, balik, terus, sahur, rasa, anji...\n",
      "3  [ini, tahun, generasi, it, bang, tidak, selese...\n",
      "4  [lagi, enak, enak, puasa, terus, mens, itu, ta...\n"
     ]
    }
   ],
   "source": [
    "normalizad_word = pd.read_excel(\"_normalisasi.xlsx\")\n",
    "normalizad_word_dict = {}\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "def normalisasi(document):\n",
    "    kalimat = document\n",
    "    for term in range(len(kalimat)):\n",
    "        if kalimat[term] in normalizad_word_dict:\n",
    "            kalimat[term] = normalizad_word_dict[kalimat[term]]\n",
    "    hasil = \" \".join(kalimat).split()\n",
    "    return hasil\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(normalisasi)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Proeses mengubah kata menjadi kata akarnya. Kode dibawah digunakan untuk mengubah kata menjadi kata\n",
    "akarnya dengan menggunakan bantuan library dari sastrawi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [tanya, minta, saran, skincare, buat, mula, ba...\n",
      "1                               [geblek, ini, orang]\n",
      "2  [habis, minum, balik, terus, sahur, rasa, anji...\n",
      "3  [ini, tahun, generasi, it, bang, tidak, selese...\n",
      "4  [lagi, enak, enak, puasa, terus, mens, itu, ta...\n"
     ]
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "term_dict = {}\n",
    "for document in TWEET_DATA['Tweet']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "def stemming(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(stemming)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "Merupakan proses untuk menghilangkan kata – kata yang sering muncul (stopword). Kode dibawah digunakan untuk menghapus stopword dari suatu teks dengan menggunakan library dari sastrawi. Teks \"kamu\",\"dia\",\"ia\",\"beliau\" dikecualikan karena kata tersebut merupakan kata ganti orang ketiga dan biasanya terdapat pada teks yang mengandung kata bullying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  [saran, skincare, banget, buta, skincare, merk...\n",
      "1                                    [geblek, orang]\n",
      "2        [habis, minum, sahur, anjing, sadar, entot]\n",
      "3  [generasi, it, bang, selese, kesal, banget, ye...\n",
      "4             [enak, enak, puasa, mens, tai, banget]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsw_INA = set(stopwords.words('indonesian'))\n",
    "lsw_INA_ex = set((\"kamu\",\"dia\",\"ia\",\"beliau\"))\n",
    "new_lsw = lsw_INA - lsw_INA_ex\n",
    "def stopword_removal(text):\n",
    "    return [word for word in text if not word in new_lsw]\n",
    "TWEET_DATA['Tweet'] = TWEET_DATA['Tweet'].apply(stopword_removal)\n",
    "print(TWEET_DATA[['Tweet']].head(5))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode dibawah digunakan untuk mengubah teks hasil preprocessing menjadi kalimat. Hal ini bertujuan agar memudahkan saat proses TF - IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  saran skincare banget buta skincare merk recom...\n",
      "1                                       geblek orang\n",
      "2               habis minum sahur anjing sadar entot\n",
      "3  generasi it bang selese kesal banget yeonjun y...\n",
      "4                    enak enak puasa mens tai banget\n"
     ]
    }
   ],
   "source": [
    "def join_text_list(texts):\n",
    "    texts = ast.literal_eval(str(texts))\n",
    "    a = ' '.join([text for text in texts])\n",
    "    return a\n",
    "TWEET_DATA[\"Tweet\"] = TWEET_DATA[\"Tweet\"].apply(join_text_list)\n",
    "print(TWEET_DATA[['Tweet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembobotan Kata\n",
    "Merupakan proses penghitungan bobot tiap kata yang dicari pada setiap dokumen sehingga dapat diketahui ketersediaan dan kemiripan suatu kata di dalam dokumen. Pembobotan dilakukan karena metode machine learning tidak bisa memproses data berupa teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF\n",
    "Merupakan salah satu metode untuk menghitung bobot kata. TF - IDF digunakan untuk menentukan nilai sebuah kata di dalam sebuah dokumen. Kode dibawah digunakan untuk menghitung nilai TF - IDF menggunakan library dari scikit-learn. Lalu hasil perhitungan ditampilkan pada sebuah dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     abang  abiz  absen  abu   ac  acap  acara  acount  adab  adam  ...   yu  \\\n",
      "0      0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "1      0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "2      0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "3      0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "4      0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "..     ...   ...    ...  ...  ...   ...    ...     ...   ...   ...  ...  ...   \n",
      "995    0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "996    0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "997    0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "998    0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "999    0.0   0.0    0.0  0.0  0.0   0.0    0.0     0.0   0.0   0.0  ...  0.0   \n",
      "\n",
      "          yuk  yuta   za  zahra  zaman  zero   zi  zidane  zodiak  \n",
      "0    0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "1    0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "2    0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "3    6.116995   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "4    0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "..        ...   ...  ...    ...    ...   ...  ...     ...     ...  \n",
      "995  0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "996  0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "997  0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "998  0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "999  0.000000   0.0  0.0    0.0    0.0   0.0  0.0     0.0     0.0  \n",
      "\n",
      "[1000 rows x 2277 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorize = TfidfVectorizer(norm=None)\n",
    "response = vectorize.fit_transform(TWEET_DATA['Tweet'])\n",
    "features_names = vectorize.get_feature_names_out()\n",
    "alist=response.todense()\n",
    "newData = pd.DataFrame(alist,columns=features_names)\n",
    "print(newData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleksi Fitur\n",
    "Merupakan proses yang melibatkan penghapusan fitur yang tidak relevan dan berulang untuk meningkatkan kinerja teknik machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "Information Gain digunakan untuk mengukur seberapa relavan suatu fitur. Fitur yang telah dihitung nilai Information Gainnya kemudian diurutkan berdasarkan nilai terbesar, lalu diseleksi menggunakan nilai threshold sebagai ambang batas. Namun untuk kode dibawah, fitur tidak akan diseleksi menggunakan nilai threshold. Karena setiap setelah tahap K - Fold Cross Validation fitur yang telah diurutkan akan dikurangi satu dari nilai Information Gain terkecil. Hal ini dilakukan hingga jumlah fitur tersisa satu. Pengurangan fitur secara berkala dilakukan untuk mencari kombinasi fitur yang paling baik. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nilai Information Gain\n",
      " abang     0.00000\n",
      "abiz      0.01034\n",
      "absen     0.00000\n",
      "abu       0.00573\n",
      "ac        0.00000\n",
      "           ...   \n",
      "zaman     0.00000\n",
      "zero      0.01358\n",
      "zi        0.00000\n",
      "zidane    0.00000\n",
      "zodiak    0.00000\n",
      "Length: 2277, dtype: float64\n",
      "\n",
      "Hasil Sorting\n",
      " kamu      0.05865\n",
      "patrol    0.05150\n",
      "kenyat    0.05136\n",
      "chika     0.04976\n",
      "tolong    0.04947\n",
      "           ...   \n",
      "kejap     0.00000\n",
      "kecoa     0.00000\n",
      "kayak     0.00000\n",
      "kaya      0.00000\n",
      "zodiak    0.00000\n",
      "Length: 2277, dtype: float64\n",
      "         kamu  patrol  kenyat  chika  tolong  info  ovo  lau  otak  way  ...  \\\n",
      "0    0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "1    0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "2    0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "3    0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "4    0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "..        ...     ...     ...    ...     ...   ...  ...  ...   ...  ...  ...   \n",
      "995  2.726971     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "996  0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "997  0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "998  0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "999  0.000000     0.0     0.0    0.0     0.0   0.0  0.0  0.0   0.0  0.0  ...   \n",
      "\n",
      "     kelelawar  kelas  kelakuanya  kekhusyuan  kekeh  kejap  kecoa  kayak  \\\n",
      "0          0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "1          0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "2          0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "3          0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "4          0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "..         ...    ...         ...         ...    ...    ...    ...    ...   \n",
      "995        0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "996        0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "997        0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "998        0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "999        0.0    0.0         0.0         0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "     kaya  zodiak  \n",
      "0     0.0     0.0  \n",
      "1     0.0     0.0  \n",
      "2     0.0     0.0  \n",
      "3     0.0     0.0  \n",
      "4     0.0     0.0  \n",
      "..    ...     ...  \n",
      "995   0.0     0.0  \n",
      "996   0.0     0.0  \n",
      "997   0.0     0.0  \n",
      "998   0.0     0.0  \n",
      "999   0.0     0.0  \n",
      "\n",
      "[1000 rows x 2277 columns]\n"
     ]
    }
   ],
   "source": [
    "col_tweet = newData.iloc[:]\n",
    "col_label = TWEET_DATA.iloc[:,-1]\n",
    "\n",
    "#menerapkan information gain\n",
    "IG_apply_skl = mutual_info_classif(col_tweet,col_label,random_state=0)\n",
    "IG = pd.Series(IG_apply_skl)\n",
    "IG.index = col_tweet.columns\n",
    "\n",
    "#bulatkan 5 angka dibelakang koma\n",
    "IG_R = round(IG,5)\n",
    "print('\\nNilai Information Gain\\n',IG_R)\n",
    "\n",
    "#sorting descend\n",
    "tinggirendah=IG_R.sort_values(ascending=False)\n",
    "tinggirendah1 = tinggirendah.index.tolist()\n",
    "\n",
    "print('\\nHasil Sorting\\n',tinggirendah)\n",
    "\n",
    "selected_columns = newData[tinggirendah1]\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K - Fold Cross Validation\n",
    "Merupakan salah satu dari jenis pengujian cross validation yang berfungsi untuk menilai kinerja proses sebuah metode algoritma dengan membagi sampel data secara acak dan mengelompokkan data tersebut sebanyak nilai k – fold. Kemudian salah satu kelompok k – fold tersebut akan dijadikan sebagai data testing sedangkan sisa kelompok yang lain akan dijadikan sebagai data training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Merupakan salah satu metode ensemble learning yang cara kerjanya menggabungkan beberapa pohon dari metode decision tree. Hasil prediksi tiap pohon dikumpulkan lalu digunakan majority voting untuk mendapatkan hasil akhir prediksi. Kode dibawah merupakan kode pengujian K - Fold Cross Validation yang didalamnya menggunakan metode machine learning Random Forest. Confusion Matrix juga digunakan untuk menguji kefektifan metode Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "temp_cm_acc = []\n",
    "temp_cm_prec = []\n",
    "temp_cm_rec = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "all_TP = []\n",
    "all_FP = []\n",
    "all_TN = []\n",
    "all_FN = []\n",
    "\n",
    "atribut = len(selected_columns.columns)\n",
    "\n",
    "for j in range(atribut,0,-1):\n",
    "    temp_new_col_tweet=selected_columns.drop(selected_columns.columns[j:], axis=1)\n",
    "    \n",
    "    v_acc_score = []\n",
    "    v_prec_score = []\n",
    "    v_rec_score = []\n",
    "\n",
    "    no=1\n",
    "\n",
    "    temp_TP_1iter = []\n",
    "    temp_FP_1iter = []\n",
    "    temp_TN_1iter = []\n",
    "    temp_FN_1iter = []\n",
    "    for train_index , test_index in kf.split(temp_new_col_tweet):\n",
    "        X_train , X_test = temp_new_col_tweet.iloc[train_index,:],temp_new_col_tweet.iloc[test_index,:]\n",
    "        y_train , y_test = col_label[train_index] , col_label[test_index]\n",
    "        \n",
    "        model.fit(X_train,y_train)#train model\n",
    "\n",
    "        pred_values = model.predict(X_test)#test model\n",
    "    \n",
    "        res = pd.DataFrame(columns=['Tweet_Split','Label_Split','Label_Prediksi'])\n",
    "        res['Tweet_Split'] = TWEET_DATA['Tweet'].iloc[test_index]\n",
    "        res['Label_Split'] = TWEET_DATA['Label'].iloc[test_index]\n",
    "        res['Label_Prediksi'] = pred_values\n",
    "       \n",
    "        totaltr = X_train.shape[0]\n",
    "        total = res.shape[0]\n",
    "        \n",
    "        for i in range(total):\n",
    "            check = res.iloc[i]\n",
    "            if(check.Label_Split=='CB' and check.Label_Prediksi=='CB'):\n",
    "                TP+=1\n",
    "            elif(check.Label_Split=='CB' and check.Label_Prediksi=='NCB'):\n",
    "                FP+=1\n",
    "            elif(check.Label_Split=='NCB' and check.Label_Prediksi=='NCB'):\n",
    "                TN+=1\n",
    "            elif(check.Label_Split=='NCB' and check.Label_Prediksi=='CB'):\n",
    "                FN+=1\n",
    "\n",
    "        temp_TP_1iter.append(TP)\n",
    "        temp_FP_1iter.append(FP)\n",
    "        temp_TN_1iter.append(TN)\n",
    "        temp_FN_1iter.append(FN)\n",
    "        try:\n",
    "            accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "           \n",
    "        res_accuracy = round(accuracy,2)\n",
    "        accuracy100=res_accuracy*100\n",
    "        \n",
    "        res_precision = round(precision,2)\n",
    "        precision100=res_precision*100\n",
    "       \n",
    "        res_recall = round(recall,2)\n",
    "        recall100 = res_recall*100\n",
    "        \n",
    "        v_acc_score.append(accuracy100)\n",
    "        v_prec_score.append(precision100)\n",
    "        v_rec_score.append(recall100)\n",
    "        \n",
    "        no=no+1\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "    avg_acc_score = sum(v_acc_score)/k\n",
    "    res_avg_acc_score = round(avg_acc_score,2)\n",
    "    temp_cm_acc.append(res_avg_acc_score)\n",
    "    \n",
    "    avg_prec_score = sum(v_prec_score)/k\n",
    "    res_avg_prec_score = round(avg_prec_score,2)\n",
    "    temp_cm_prec.append(res_avg_prec_score)\n",
    "\n",
    "    avg_rec_score = sum(v_rec_score)/k\n",
    "    res_avg_rec_score = round(avg_rec_score,2)\n",
    "    temp_cm_rec.append(res_avg_rec_score)\n",
    "\n",
    "    print(\"\\nRata - Rata Confusion Matrix Menggunakan\",atribut,\"atribut\")\n",
    "    print(\"Akurasi =\",res_avg_acc_score)\n",
    "    print(\"Presisi =\",res_avg_prec_score)\n",
    "    print(\"Recall =\",res_avg_rec_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode dibawah merupakan kode untuk menampilkan rata - rata Confusion Matrix hasil K - Fold Cross Validation dengan pengurangan fitur secara berakala dalam tampilan grafis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = max(temp_cm_acc)\n",
    "best_prec = max(temp_cm_prec)\n",
    "best_rec = max(temp_cm_rec)\n",
    "\n",
    "temp_cm_acc.reverse()\n",
    "temp_cm_prec.reverse()\n",
    "temp_cm_rec.reverse()\n",
    "\n",
    "var_acc = temp_cm_acc.index(best_acc)+1\n",
    "var_prec = temp_cm_prec.index(best_prec)+1\n",
    "var_rec = temp_cm_rec.index(best_rec)+1\n",
    "\n",
    "len_temp_cm_acc = len(temp_cm_acc)\n",
    "\n",
    "length = len_temp_cm_acc/2\n",
    "wide = len_temp_cm_acc/3\n",
    "\n",
    "wew = [x+1 for x in range(len_temp_cm_acc)]\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(20, 10,forward=True)\n",
    "fig.set_dpi(100)\n",
    "\n",
    "x_plot = np.array([x+1 for x in range(len_temp_cm_acc)])\n",
    "\n",
    "plt.xlabel('Jumlah Atribut',fontsize=30)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "y_plot_acc = np.array(temp_cm_acc)\n",
    "y_plot_prec = np.array(temp_cm_prec)\n",
    "y_plot_rec = np.array(temp_cm_rec)\n",
    "\n",
    "panjang=len(x_plot)\n",
    "panjang1=panjang//5\n",
    "cek1=x_plot[:panjang1]\n",
    "cek2=x_plot[-panjang1:]\n",
    "\n",
    "\n",
    "if(var_acc in cek1):\n",
    "    sumbuX=var_acc+10\n",
    "elif(var_acc in cek1):\n",
    "    sumbuX=var_acc-10\n",
    "else:\n",
    "    sumbuX=var_acc\n",
    "    \n",
    "if(var_prec in cek1):\n",
    "    sumbuX=var_prec+10\n",
    "elif(var_prec in cek1):\n",
    "    sumbuX=var_prec-10\n",
    "else:\n",
    "    sumbuX=var_prec\n",
    "        \n",
    "if(var_rec in cek1):\n",
    "    sumbuX=var_rec+10\n",
    "elif(var_rec in cek1):\n",
    "    sumbuX=var_rec-10\n",
    "else:\n",
    "    sumbuX=var_rec\n",
    "        \n",
    "\n",
    "kalimat_acc = str(var_acc)+\" Atribut: \"+str(best_acc)+\"%\"\n",
    "plt.annotate(kalimat_acc, xy =(var_acc, best_acc),\n",
    "                xytext =(var_acc, best_acc+4),arrowprops=dict(facecolor='black', shrink=0.03),\n",
    "                ha='center', va='bottom',fontsize=30)\n",
    "\n",
    "kalimat_prec = str(var_prec)+\" Atribut: \"+str(best_prec)+\"%\"\n",
    "plt.annotate(kalimat_prec, xy =(var_prec, best_prec),\n",
    "                xytext =(var_prec, best_prec+4),arrowprops=dict(facecolor='black', shrink=0.03),\n",
    "                ha='center', va='bottom',fontsize=30)\n",
    "\n",
    "kalimat_rec = str(var_rec)+\" Atribut: \"+str(best_rec)+\"%\"\n",
    "plt.annotate(kalimat_rec, xy =(var_rec, best_rec),\n",
    "                xytext =(var_rec, best_rec+4),arrowprops=dict(facecolor='black', shrink=0.03),\n",
    "                ha='center', va='bottom',fontsize=30)\n",
    "\n",
    "\n",
    "plt.ylabel('Nilai',fontsize=30)\n",
    "plt.plot(x_plot, y_plot_acc, color='c', label='Accuracy')\n",
    "plt.plot(x_plot, y_plot_prec,color='m', label='Precision')\n",
    "plt.plot(x_plot, y_plot_rec,color='y', label='Recall')\n",
    "\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=30)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
